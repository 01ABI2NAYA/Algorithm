{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157191e2",
   "metadata": {},
   "source": [
    "# WHITE BOX IN NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897fa213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Mean Squared Error: 0.38021281086122827\n",
      "Epoch 1000, Mean Squared Error: 0.2535191514955752\n",
      "Epoch 2000, Mean Squared Error: 0.25079634258097566\n",
      "Epoch 3000, Mean Squared Error: 0.24854235024886856\n",
      "Epoch 4000, Mean Squared Error: 0.24634364512123708\n",
      "Epoch 5000, Mean Squared Error: 0.24390176705595107\n",
      "Epoch 6000, Mean Squared Error: 0.24100319758956967\n",
      "Epoch 7000, Mean Squared Error: 0.23750199223190277\n",
      "Epoch 8000, Mean Squared Error: 0.23330206932919892\n",
      "Epoch 9000, Mean Squared Error: 0.2283516194325636\n",
      "Predictions after training:\n",
      "[[0.41113868]\n",
      " [0.56515236]\n",
      " [0.51410005]\n",
      " [0.54444091]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WhiteBoxNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "        # Learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        # Input to hidden layer\n",
    "        self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
    "\n",
    "        # Hidden to output layer\n",
    "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output_layer_output = self.sigmoid(self.output_layer_input)\n",
    "\n",
    "        return self.output_layer_output\n",
    "\n",
    "    def backward_propagation(self, inputs, targets):\n",
    "        # Output layer error and delta\n",
    "        output_error = targets - self.output_layer_output\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output_layer_output)\n",
    "\n",
    "        # Hidden layer error and delta\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_layer_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_delta) * self.learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.weights_input_hidden += inputs.T.dot(hidden_delta) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, inputs, targets, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward and backward propagation\n",
    "            predictions = self.forward_propagation(inputs)\n",
    "            self.backward_propagation(inputs, targets)\n",
    "\n",
    "            # Print mean squared error for monitoring\n",
    "            mse = np.mean((targets - predictions) ** 2)\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Mean Squared Error: {mse}\")\n",
    "\n",
    "# Example Usage:\n",
    "# Define input, output, and hidden layer sizes\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Create an instance of the white-box neural network\n",
    "nn = WhiteBoxNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Training data (example dataset)\n",
    "training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "training_targets = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train the neural network\n",
    "nn.train(training_inputs, training_targets, epochs=10000)\n",
    "\n",
    "# Test the trained neural network\n",
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "predictions = nn.forward_propagation(test_data)\n",
    "print(\"Predictions after training:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd6ed1",
   "metadata": {},
   "source": [
    "# Activation function: MISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88858b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define Mish activation function\n",
    "class Mish(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build the neural network with Mish activation\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation=Mish()),\n",
    "    layers.Dense(64, activation=Mish()),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e2c96",
   "metadata": {},
   "source": [
    "# Ant colony optimization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67103c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define a simple neural network model\n",
    "def create_neural_network(input_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_shape=(input_size,)))\n",
    "    model.add(layers.Dense(1, activation='linear'))  # Adjust for your specific problem\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Ant Colony Optimization\n",
    "class AntColony:\n",
    "    def __init__(self, num_ants, input_size):\n",
    "        self.num_ants = num_ants\n",
    "        self.input_size = input_size\n",
    "        self.neural_network = create_neural_network(input_size)\n",
    "        self.best_solution = None\n",
    "        self.best_solution_score = np.inf\n",
    "\n",
    "    def generate_solution(self):\n",
    "        # Implement your ACO solution generation logic here\n",
    "        # Use the neural network to evaluate the quality of solutions\n",
    "        solution = np.random.rand(self.input_size)  # Placeholder, replace with your logic\n",
    "        score = self.neural_network.predict(solution.reshape(1, -1))[0][0]\n",
    "        return solution, score\n",
    "\n",
    "    def search(self, num_iterations):\n",
    "        for _ in range(num_iterations):\n",
    "            for ant in range(self.num_ants):\n",
    "                solution, score = self.generate_solution()\n",
    "                if score < self.best_solution_score:\n",
    "                    self.best_solution = solution\n",
    "                    self.best_solution_score = score\n",
    "        return self.best_solution\n",
    "\n",
    "# Example Usage\n",
    "input_size = 10  # Replace with the size of your problem\n",
    "num_ants = 10\n",
    "num_iterations = 10\n",
    "\n",
    "aco = AntColony(num_ants, input_size)\n",
    "best_solution = aco.search(num_iterations)\n",
    "\n",
    "print(\"Best Solution:\", best_solution)\n",
    "print(\"Best Solution Score:\", aco.best_solution_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da665f",
   "metadata": {},
   "source": [
    "# Backward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b967dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "bias_hidden = np.zeros((1, hidden_size))\n",
    "bias_output = np.zeros((1, output_size))\n",
    "\n",
    "# Input data and labels\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Training the neural network using backpropagation\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Compute the loss\n",
    "    error = y - predicted_output\n",
    "\n",
    "    # Backpropagation\n",
    "    output_error = error * sigmoid_derivative(predicted_output)\n",
    "    hidden_layer_error = output_error.dot(weights_hidden_output.T) * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(output_error) * learning_rate\n",
    "    weights_input_hidden += X.T.dot(hidden_layer_error) * learning_rate\n",
    "    bias_output += np.sum(output_error, axis=0, keepdims=True) * learning_rate\n",
    "    bias_hidden += np.sum(hidden_layer_error, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "# Test the trained neural network\n",
    "test_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "test_hidden = sigmoid(np.dot(test_input, weights_input_hidden) + bias_hidden)\n",
    "test_output = sigmoid(np.dot(test_hidden, weights_hidden_output) + bias_output)\n",
    "\n",
    "print(\"Predicted Output:\")\n",
    "print(test_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2fe88",
   "metadata": {},
   "source": [
    "# SIMPLE NEURAL NETWORK FOR DIFFERENT ACTIVATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cf3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Function to create a neural network with specified activation function\n",
    "def create_model(activation_function):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=activation_function, input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=activation_function))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=activation_function))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# List of activation functions to try\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh', 'softmax']\n",
    "\n",
    "# Train and evaluate models with different activation functions\n",
    "for activation_function in activation_functions:\n",
    "    model = create_model(activation_function)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print(f'\\nModel with {activation_function} activation function:')\n",
    "    print(f'Test accuracy: {test_acc:.4f}\\n')\n",
    "\n",
    "    # Plot training history\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Model with {activation_function} Activation Function')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2daa8",
   "metadata": {},
   "source": [
    "# GENETIC ALGORITHM IN NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f376ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to initialize a population of neural networks\n",
    "def initialize_population(population_size, input_dim):\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        model = create_model(input_dim)\n",
    "        population.append(model)\n",
    "    return population\n",
    "\n",
    "# Function to evaluate the fitness of a neural network on the given dataset\n",
    "def evaluate_fitness(model, X, y):\n",
    "    _, accuracy = model.evaluate(X, y, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Function to select parents based on tournament selection\n",
    "def select_parents(population, fitness, num_parents):\n",
    "    parents = []\n",
    "    for _ in range(num_parents):\n",
    "        tournament_indices = np.random.choice(len(population), size=5, replace=False)\n",
    "        tournament_fitness = [fitness[i] for i in tournament_indices]\n",
    "        selected_index = tournament_indices[np.argmax(tournament_fitness)]\n",
    "        parents.append(population[selected_index])\n",
    "    return parents\n",
    "\n",
    "# Function to perform crossover (single-point crossover in this example)\n",
    "def crossover(parent1, parent2):\n",
    "    child = create_model(parent1.input_shape[1])\n",
    "    weights_parent1 = parent1.get_weights()\n",
    "    weights_parent2 = parent2.get_weights()\n",
    "    crossover_point = np.random.randint(len(weights_parent1))\n",
    "    child.set_weights(weights_parent1[:crossover_point] + weights_parent2[crossover_point:])\n",
    "    return child\n",
    "\n",
    "# Function to perform mutation (randomly change weights)\n",
    "def mutate(child, mutation_rate=0.1):\n",
    "    weights_child = child.get_weights()\n",
    "    for i in range(len(weights_child)):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            weights_child[i] = np.random.randn(*weights_child[i].shape)\n",
    "    child.set_weights(weights_child)\n",
    "    return child\n",
    "\n",
    "# Genetic Algorithm\n",
    "population_size = 10\n",
    "num_generations = 10\n",
    "\n",
    "# Initialize the population\n",
    "population = initialize_population(population_size, X_train.shape[1])\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate fitness of each individual in the population\n",
    "    fitness_scores = [evaluate_fitness(model, X_train, y_train) for model in population]\n",
    "\n",
    "    # Select parents\n",
    "    parents = select_parents(population, fitness_scores, num_parents=2)\n",
    "\n",
    "    # Create offspring through crossover and mutation\n",
    "    offspring = []\n",
    "    for _ in range(population_size - len(parents)):\n",
    "        parent1, parent2 = np.random.choice(parents, size=2, replace=False)\n",
    "        child = crossover(parent1, parent2)\n",
    "        child = mutate(child)\n",
    "        offspring.append(child)\n",
    "\n",
    "    # Replace the old population with the new population\n",
    "    population = parents + offspring\n",
    "\n",
    "# Evaluate the best individual in the final population on the test set\n",
    "best_individual = max(population, key=lambda model: evaluate_fitness(model, X_test, y_test))\n",
    "test_accuracy = evaluate_fitness(best_individual, X_test, y_test)\n",
    "print(f\"Test Accuracy of the Best Individual: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded914a5",
   "metadata": {},
   "source": [
    "# GREY WOLF OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfcb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape((X_train.shape[0], -1)).astype('float32') / 255.0\n",
    "X_test = X_test.reshape((X_test.shape[0], -1)).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Function to create a simple neural network model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the fitness of the neural network\n",
    "def evaluate_fitness(model, X, y):\n",
    "    _, accuracy = model.evaluate(X, y, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Grey Wolf Optimization algorithm\n",
    "def grey_wolf_optimization(X_train, y_train, num_search_agents=5, num_iterations=50):\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Initialize the alpha, beta, and delta positions and fitness values\n",
    "    alpha_position = np.random.rand(input_dim)\n",
    "    alpha_fitness = evaluate_fitness(model, X_train, y_train)\n",
    "    beta_position, beta_fitness = np.copy(alpha_position), alpha_fitness\n",
    "    delta_position, delta_fitness = np.copy(alpha_position), alpha_fitness\n",
    "\n",
    "    # Initialize the search agents\n",
    "    wolves_positions = np.random.rand(num_search_agents, input_dim)\n",
    "    wolves_fitness = np.zeros(num_search_agents)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        for i in range(num_search_agents):\n",
    "            # Update fitness for each search agent\n",
    "            fitness = evaluate_fitness(model, X_train, y_train)\n",
    "            wolves_fitness[i] = fitness\n",
    "\n",
    "            # Update alpha, beta, and delta positions\n",
    "            if fitness > alpha_fitness:\n",
    "                alpha_position = np.copy(wolves_positions[i])\n",
    "                alpha_fitness = fitness\n",
    "            elif fitness > beta_fitness:\n",
    "                beta_position = np.copy(wolves_positions[i])\n",
    "                beta_fitness = fitness\n",
    "            elif fitness > delta_fitness:\n",
    "                delta_position = np.copy(wolves_positions[i])\n",
    "                delta_fitness = fitness\n",
    "\n",
    "        # Update the positions of the search agents using Grey Wolf Algorithm\n",
    "        a = 2 - 2 * iteration / num_iterations  # Linearly decreasing alpha\n",
    "        for i in range(num_search_agents):\n",
    "            r1, r2 = np.random.rand(input_dim), np.random.rand(input_dim)\n",
    "            A1, C1 = 2 * a * r1 - a, 2 * r2\n",
    "            D_alpha = np.abs(C1 * alpha_position - wolves_positions[i])\n",
    "            X1 = alpha_position - A1 * D_alpha\n",
    "            wolves_positions[i] = X1\n",
    "\n",
    "    # Combine weights and biases\n",
    "    best_weights = np.concatenate([alpha_position, np.zeros(alpha_position.shape)])\n",
    "\n",
    "    return best_weights\n",
    "\n",
    "# Create and compile the neural network model\n",
    "model = create_model(X_train.shape[1])\n",
    "\n",
    "# Apply Grey Wolf Optimization to train the neural network\n",
    "best_weights = grey_wolf_optimization(X_train, y_train, num_search_agents=5, num_iterations=50)\n",
    "\n",
    "# Set the best weights to the neural network model\n",
    "model.set_weights([best_weights])\n",
    "\n",
    "# Evaluate the neural network on the test set\n",
    "accuracy = evaluate_fitness(model, X_test, y_test)\n",
    "print(f\"Accuracy on the test set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181d880",
   "metadata": {},
   "source": [
    "# PARTICLE SWARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd06ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"74e7ebc2\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Iteration 1/100\\n\",\n",
    "      \"Iteration 2/100\\n\",\n",
    "      \"Iteration 3/100\\n\",\n",
    "      \"Iteration 4/100\\n\",\n",
    "      \"Iteration 5/100\\n\",\n",
    "      \"Iteration 6/100\\n\",\n",
    "      \"Iteration 7/100\\n\",\n",
    "      \"Iteration 8/100\\n\",\n",
    "      \"Iteration 9/100\\n\",\n",
    "      \"Iteration 10/100\\n\",\n",
    "      \"Iteration 11/100\\n\",\n",
    "      \"Iteration 12/100\\n\",\n",
    "      \"Iteration 13/100\\n\",\n",
    "      \"Iteration 14/100\\n\",\n",
    "      \"Iteration 15/100\\n\",\n",
    "      \"Iteration 16/100\\n\",\n",
    "      \"Iteration 17/100\\n\",\n",
    "      \"Iteration 18/100\\n\",\n",
    "      \"Iteration 19/100\\n\",\n",
    "      \"Iteration 20/100\\n\",\n",
    "      \"Iteration 21/100\\n\",\n",
    "      \"Iteration 22/100\\n\",\n",
    "      \"Iteration 23/100\\n\",\n",
    "      \"Iteration 24/100\\n\",\n",
    "      \"Iteration 25/100\\n\",\n",
    "      \"Iteration 26/100\\n\",\n",
    "      \"Iteration 27/100\\n\",\n",
    "      \"Iteration 28/100\\n\",\n",
    "      \"Iteration 29/100\\n\",\n",
    "      \"Iteration 30/100\\n\",\n",
    "      \"Iteration 31/100\\n\",\n",
    "      \"Iteration 32/100\\n\",\n",
    "      \"Iteration 33/100\\n\",\n",
    "      \"Iteration 34/100\\n\",\n",
    "      \"Iteration 35/100\\n\",\n",
    "      \"Iteration 36/100\\n\",\n",
    "      \"Iteration 37/100\\n\",\n",
    "      \"Iteration 38/100\\n\",\n",
    "      \"Iteration 39/100\\n\",\n",
    "      \"Iteration 40/100\\n\",\n",
    "      \"Iteration 41/100\\n\",\n",
    "      \"Iteration 42/100\\n\",\n",
    "      \"Iteration 43/100\\n\",\n",
    "      \"Iteration 44/100\\n\",\n",
    "      \"Iteration 45/100\\n\",\n",
    "      \"Iteration 46/100\\n\",\n",
    "      \"Iteration 47/100\\n\",\n",
    "      \"Iteration 48/100\\n\",\n",
    "      \"Iteration 49/100\\n\",\n",
    "      \"Iteration 50/100\\n\",\n",
    "      \"Iteration 51/100\\n\",\n",
    "      \"Iteration 52/100\\n\",\n",
    "      \"Iteration 53/100\\n\",\n",
    "      \"Iteration 54/100\\n\",\n",
    "      \"Iteration 55/100\\n\",\n",
    "      \"Iteration 56/100\\n\",\n",
    "      \"Iteration 57/100\\n\",\n",
    "      \"Iteration 58/100\\n\",\n",
    "      \"Iteration 59/100\\n\",\n",
    "      \"Iteration 60/100\\n\",\n",
    "      \"Iteration 61/100\\n\",\n",
    "      \"Iteration 62/100\\n\",\n",
    "      \"Iteration 63/100\\n\",\n",
    "      \"Iteration 64/100\\n\",\n",
    "      \"Iteration 65/100\\n\",\n",
    "      \"Iteration 66/100\\n\",\n",
    "      \"Iteration 67/100\\n\",\n",
    "      \"Iteration 68/100\\n\",\n",
    "      \"Iteration 69/100\\n\",\n",
    "      \"Iteration 70/100\\n\",\n",
    "      \"Iteration 71/100\\n\",\n",
    "      \"Iteration 72/100\\n\",\n",
    "      \"Iteration 73/100\\n\",\n",
    "      \"Iteration 74/100\\n\",\n",
    "      \"Iteration 75/100\\n\",\n",
    "      \"Iteration 76/100\\n\",\n",
    "      \"Iteration 77/100\\n\",\n",
    "      \"Iteration 78/100\\n\",\n",
    "      \"Iteration 79/100\\n\",\n",
    "      \"Iteration 80/100\\n\",\n",
    "      \"Iteration 81/100\\n\",\n",
    "      \"Iteration 82/100\\n\",\n",
    "      \"Iteration 83/100\\n\",\n",
    "      \"Iteration 84/100\\n\",\n",
    "      \"Iteration 85/100\\n\",\n",
    "      \"Iteration 86/100\\n\",\n",
    "      \"Iteration 87/100\\n\",\n",
    "      \"Iteration 88/100\\n\",\n",
    "      \"Iteration 89/100\\n\",\n",
    "      \"Iteration 90/100\\n\",\n",
    "      \"Iteration 91/100\\n\",\n",
    "      \"Iteration 92/100\\n\",\n",
    "      \"Iteration 93/100\\n\",\n",
    "      \"Iteration 94/100\\n\",\n",
    "      \"Iteration 95/100\\n\",\n",
    "      \"Iteration 96/100\\n\",\n",
    "      \"Iteration 97/100\\n\",\n",
    "      \"Iteration 98/100\\n\",\n",
    "      \"Iteration 99/100\\n\",\n",
    "      \"Iteration 100/100\\n\",\n",
    "      \"Predictions:\\n\",\n",
    "      \"[[0.96452047]\\n\",\n",
    "      \" [0.97708271]\\n\",\n",
    "      \" [0.980095  ]\\n\",\n",
    "      \" [0.98323748]]\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define the sigmoid activation function\\n\",\n",
    "    \"def sigmoid(x):\\n\",\n",
    "    \"    return 1 / (1 + np.exp(-x))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define the neural network class\\n\",\n",
    "    \"class NeuralNetwork:\\n\",\n",
    "    \"    def __init__(self, input_size, hidden_size, output_size):\\n\",\n",
    "    \"        # Initialize weights and biases\\n\",\n",
    "    \"        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\\n\",\n",
    "    \"        self.bias_hidden = np.zeros((1, hidden_size))\\n\",\n",
    "    \"        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\\n\",\n",
    "    \"        self.bias_output = np.zeros((1, output_size))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, inputs):\\n\",\n",
    "    \"        # Forward propagation\\n\",\n",
    "    \"        self.hidden_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\\n\",\n",
    "    \"        self.hidden_output = sigmoid(self.hidden_input)\\n\",\n",
    "    \"        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\\n\",\n",
    "    \"        return sigmoid(self.output)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def mse_loss(self, predictions, targets):\\n\",\n",
    "    \"        return np.mean((targets - predictions) ** 2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def get_parameters(self):\\n\",\n",
    "    \"        return np.concatenate([\\n\",\n",
    "    \"            self.weights_input_hidden.flatten(),\\n\",\n",
    "    \"            self.bias_hidden.flatten(),\\n\",\n",
    "    \"            self.weights_hidden_output.flatten(),\\n\",\n",
    "    \"            self.bias_output.flatten()\\n\",\n",
    "    \"        ])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def set_parameters(self, parameters):\\n\",\n",
    "    \"        input_hidden_end = self.weights_input_hidden.size\\n\",\n",
    "    \"        hidden_bias_end = input_hidden_end + self.bias_hidden.size\\n\",\n",
    "    \"        hidden_output_end = hidden_bias_end + self.weights_hidden_output.size\\n\",\n",
    "    \"\\n\",\n",
    "    \"        self.weights_input_hidden = parameters[:input_hidden_end].reshape(self.weights_input_hidden.shape)\\n\",\n",
    "    \"        self.bias_hidden = parameters[input_hidden_end:hidden_bias_end].reshape(self.bias_hidden.shape)\\n\",\n",
    "    \"        self.weights_hidden_output = parameters[hidden_bias_end:hidden_output_end].reshape(self.weights_hidden_output.shape)\\n\",\n",
    "    \"        self.bias_output = parameters[hidden_output_end:].reshape(self.bias_output.shape)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define the Particle class\\n\",\n",
    "    \"class Particle:\\n\",\n",
    "    \"    def __init__(self, dimension):\\n\",\n",
    "    \"        self.position = np.random.rand(dimension)\\n\",\n",
    "    \"        self.velocity = np.random.rand(dimension)\\n\",\n",
    "    \"        self.best_position = self.position\\n\",\n",
    "    \"        self.best_fitness = float('-inf')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define the Particle Swarm Optimization class\\n\",\n",
    "    \"class ParticleSwarmOptimization:\\n\",\n",
    "    \"    def __init__(self, population_size, inertia_weight, cognitive_coefficient, social_coefficient, max_iterations):\\n\",\n",
    "    \"        self.population_size = population_size\\n\",\n",
    "    \"        self.inertia_weight = inertia_weight\\n\",\n",
    "    \"        self.cognitive_coefficient = cognitive_coefficient\\n\",\n",
    "    \"        self.social_coefficient = social_coefficient\\n\",\n",
    "    \"        self.max_iterations = max_iterations\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def initialize_population(self, dimension):\\n\",\n",
    "    \"        return [Particle(dimension) for _ in range(self.population_size)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def update_velocity(self, particle, global_best_position):\\n\",\n",
    "    \"        inertia_term = self.inertia_weight * particle.velocity\\n\",\n",
    "    \"        cognitive_term = self.cognitive_coefficient * np.random.rand() * (particle.best_position - particle.position)\\n\",\n",
    "    \"        social_term = self.social_coefficient * np.random.rand() * (global_best_position - particle.position)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        new_velocity = inertia_term + cognitive_term + social_term\\n\",\n",
    "    \"        return new_velocity\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Example usage\\n\",\n",
    "    \"# Assuming you have input data `X` and target data `y`\\n\",\n",
    "    \"# Make sure to normalize your input data before training\\n\",\n",
    "    \"# (e.g., dividing by the maximum value or using z-score normalization)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define input and target data\\n\",\n",
    "    \"X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\\n\",\n",
    "    \"y = np.array([[0], [1], [1], [0]])\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_size = 2\\n\",\n",
    "    \"hidden_size = 4\\n\",\n",
    "    \"output_size = 1\\n\",\n",
    "    \"chromosome_length = (input_size * hidden_size) + hidden_size + (hidden_size * output_size) + output_size\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a neural network\\n\",\n",
    "    \"nn = NeuralNetwork(input_size, hidden_size, output_size)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a Particle Swarm Optimization algorithm\\n\",\n",
    "    \"population_size = 10\\n\",\n",
    "    \"inertia_weight = 0.5\\n\",\n",
    "    \"cognitive_coefficient = 2.0\\n\",\n",
    "    \"social_coefficient = 2.0\\n\",\n",
    "    \"max_iterations = 100\\n\",\n",
    "    \"\\n\",\n",
    "    \"pso = ParticleSwarmOptimization(population_size, inertia_weight, cognitive_coefficient, social_coefficient, max_iterations)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize population\\n\",\n",
    "    \"particles = pso.initialize_population(chromosome_length)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train the neural network using Particle Swarm Optimization\\n\",\n",
    "    \"for iteration in range(max_iterations):\\n\",\n",
    "    \"    print(f\\\"Iteration {iteration + 1}/{max_iterations}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for particle in particles:\\n\",\n",
    "    \"        nn.set_parameters(particle.position)\\n\",\n",
    "    \"        predictions = nn.forward(X)\\n\",\n",
    "    \"        fitness = 1 / nn.mse_loss(predictions, y)  # Fitness is inverse of MSE\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if fitness > particle.best_fitness:\\n\",\n",
    "    \"            particle.best_fitness = fitness\\n\",\n",
    "    \"            particle.best_position = particle.position\\n\",\n",
    "    \"\\n\",\n",
    "    \"    global_best_particle = max(particles, key=lambda x: x.best_fitness).best_position\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for particle in particles:\\n\",\n",
    "    \"        new_velocity = pso.update_velocity(particle, global_best_particle)\\n\",\n",
    "    \"        particle.velocity = new_velocity\\n\",\n",
    "    \"        particle.position += particle.velocity\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select the best individual from the final population\\n\",\n",
    "    \"best_particle = max(particles, key=lambda x: x.best_fitness)\\n\",\n",
    "    \"nn.set_parameters(best_particle.best_position)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test the trained network\\n\",\n",
    "    \"predictions = nn.forward(X)\\n\",\n",
    "    \"print(\\\"Predictions:\\\")\\n\",\n",
    "    \"print(predictions)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"d4aced6e\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c53bb",
   "metadata": {},
   "source": [
    "# DIFFERENT TYPES OF SEARCH ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8c6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path from S to F: S -> A -> F\n"
     ]
    }
   ],
   "source": [
    "# Graph representation as an adjacency list\n",
    "graph = {\n",
    "    'S': ['A', 'B'],\n",
    "    'A': ['S', 'F'],\n",
    "    'B': ['S', 'C', 'D'],\n",
    "    'D': ['B', 'F'],\n",
    "    'C': ['B', 'E'],\n",
    "    'E': ['C', 'F'],\n",
    "    'F': ['A', 'D', 'E']\n",
    "}\n",
    "\n",
    "def heuristic(node, goal):\n",
    "    # Uniform heuristic function (same for all nodes)\n",
    "    return 1\n",
    "\n",
    "def astar_search(graph, start, goal):\n",
    "    open_list = [(start, [start], 0)]\n",
    "    while open_list:\n",
    "        node, path, cost_so_far = open_list.pop(0)\n",
    "\n",
    "        if node == goal:\n",
    "            return path\n",
    "\n",
    "        for neighbor in graph.get(node, []):\n",
    "            new_path = path + [neighbor]\n",
    "            new_cost = cost_so_far + heuristic(neighbor, goal)\n",
    "            open_list.append((neighbor, new_path, new_cost))\n",
    "\n",
    "        # Sort the open list by the total cost (cost_so_far + heuristic)\n",
    "        open_list.sort(key=lambda x: x[2] + heuristic(x[0], goal))\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "start_node = 'S'\n",
    "goal_node = 'F'\n",
    "path = astar_search(graph, start_node, goal_node)\n",
    "\n",
    "if path:\n",
    "    print(f\"Path from {start_node} to {goal_node}: {' -> '.join(path)}\")\n",
    "else:\n",
    "    print(f\"No path found from {start_node} to {goal_node}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5812e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F']\n"
     ]
    }
   ],
   "source": [
    "def beam_search(graph, start_node, goal_node, beam_width):\n",
    "    queue = [(start_node, 0)]\n",
    "    closed_set = set()\n",
    "    while queue:\n",
    "        node, cost = queue.pop(0)\n",
    "        if node == goal_node:\n",
    "            return [node]\n",
    "        closed_set.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in closed_set:\n",
    "                queue.append((neighbor, cost + 1))\n",
    "        if len(queue) == beam_width:\n",
    "            worst_cost = queue.pop(0)\n",
    "            if worst_cost != 0:\n",
    "                closed_set.remove(worst_cost)\n",
    "    return None\n",
    "graph = {\n",
    "    'S': ['A', 'B'],\n",
    "    'A': ['S', 'F'],\n",
    "    'B': ['S', 'C', 'D'],\n",
    "    'D': ['B', 'F'],\n",
    "    'C': ['B', 'E'],\n",
    "    'E': ['C', 'F'],\n",
    "    'F': ['A', 'D', 'E']\n",
    "}\n",
    "\n",
    "start_node = 'S'\n",
    "goal_node = 'F'\n",
    "beam_width= 1\n",
    "path = beam_search(graph, start_node, goal_node, beam_width)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a066306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path from A to G: A -> D -> K -> M -> O -> G\n"
     ]
    }
   ],
   "source": [
    "# Graph representation as an adjacency list with edge costs\n",
    "graph = {\n",
    "    'S': [('A', 4),('B', 6),('C',3)],\n",
    "    'A':[('D',8),('E',7)],\n",
    "    'D':[('K',2)],\n",
    "    'E':[('M',5)],\n",
    "    'K':[('M',1)],\n",
    "    'M':[('O',4)],\n",
    "    'O':[('G',5)],\n",
    "    'B':[('F',4)],\n",
    "    'F':[('O',3)],\n",
    "    'B':[('G',7)],\n",
    "    'C':[('H',9)],\n",
    "    'H':[('G',2)],\n",
    "    'C':[('J',8)],\n",
    "    'J':[('R',5)],\n",
    "    'R':[('T',3)],\n",
    "    'J':[('T',4)],\n",
    "}\n",
    "\n",
    "def best_first_search(graph, start, goal):\n",
    "    open_list = [(0, [start])]\n",
    "    while open_list:\n",
    "        open_list.sort(key=lambda x: x[0])  # Sort by cost\n",
    "        cost, path = open_list.pop(0)\n",
    "\n",
    "        node = path[-1]\n",
    "        if node == goal:\n",
    "            return path\n",
    "\n",
    "        for neighbor, edge_cost in graph.get(node, []):\n",
    "            if neighbor not in path:\n",
    "                new_cost = cost + edge_cost\n",
    "                new_path = path + [neighbor]\n",
    "                open_list.append((new_cost, new_path))\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "start_node = 'A'\n",
    "goal_node = 'G'\n",
    "path = best_first_search(graph, start_node, goal_node)\n",
    "print(f\"Path from {start_node} to {goal_node}: {' -> '.join(path)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5dceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No path found from D to G.\n"
     ]
    }
   ],
   "source": [
    "# Graph representation as an adjacency list with edge costs\n",
    "graph = {\n",
    "    'S': [('A', 1)],\n",
    "    'A':[('B',3)],\n",
    "    'B':[('C',1)],\n",
    "    'C':[('G',1)],\n",
    "    'B':[('G',1)],\n",
    "    'S':[('D',5)],\n",
    "    'D':[('G',1)],\n",
    "    'G':[]\n",
    "}\n",
    "\n",
    "\n",
    "def depth_first_search(graph, node, goal, path=None):\n",
    "    if path is None:\n",
    "        path = []\n",
    "    path = path + [node]\n",
    "\n",
    "    if node == goal:\n",
    "        return path\n",
    "\n",
    "    if node not in graph:\n",
    "        return None\n",
    "\n",
    "    lowest_cost_path = None\n",
    "    for neighbor in graph[node]:\n",
    "        if neighbor not in path:\n",
    "            new_path = depth_first_search(graph, neighbor, goal, path)\n",
    "            if new_path:\n",
    "                if lowest_cost_path is None or len(new_path) < len(lowest_cost_path):\n",
    "                    lowest_cost_path = new_path\n",
    "\n",
    "    return lowest_cost_path\n",
    "\n",
    "# Example usage\n",
    "start_node = 'D'\n",
    "goal_node = 'G'\n",
    "path = depth_first_search(graph, start_node, goal_node)\n",
    "\n",
    "if path:\n",
    "    print(f\"Path from {start_node} to {goal_node}: {' -> '.join(path)}\")\n",
    "else:\n",
    "    print(f\"No path found from {start_node} to {goal_node}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da3ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
